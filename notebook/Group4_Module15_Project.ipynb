{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Trustworthy AI Explainer Dashboard: Interactive LLM-based Tutor with Explainability and Feedback\n",
        "**Module 15 Team Project**\n",
        "\n",
        "---\n",
        "\n",
        "**Team Name:** Group4\n",
        "\n",
        "**Team Members:**\n",
        "1. Member A (PM/Integrator)\n",
        "2. Member B (Gradio)\n",
        "3. Member C (Streamlit)\n",
        "4. Member D (Explainability)\n",
        "5. Member E (Deployment/Docs)\n",
        "\n",
        "**Institution:** CITEDI\n",
        "\n",
        "**Date:** 05 Feb 2026\n",
        "\n",
        "---\n",
        "\n",
        "## Project overview\n",
        "\n",
        "**Note: This is a team project. All team members should collaborate on all sections.**\n",
        "\n",
        "This team project focuses on building user interfaces for LLM applications using Gradio and Streamlit. Your team will:\n",
        "\n",
        "1. **UI framework understanding** (Gradio vs Streamlit) with clear trade-offs\n",
        "2. **Gradio prototype** (chat + memory) deployed on Hugging Face Spaces\n",
        "3. **Streamlit dashboard** deployed on Streamlit Community Cloud\n",
        "4. **Chatbot interface** powered by OpenAI API with conversational memory\n",
        "5. **Explainability integration** using **SHAP (global)** + **LIME (local)** alongside LLM outputs\n",
        "6. **Feedback mechanism** for users to rate/flag outputs (logged to CSV)\n",
        "7. **State & caching management** for performance and stable UX\n",
        "8. **Production deployment** with secure secrets management (no hardcoded keys)\n",
        "\n",
        "**Estimated completion time:** 80 minutes (per team member)\n",
        "\n",
        "**Note:** This notebook contains learning materials and documentation. Final deliverables are the Python apps:\n",
        "- `Group4_Module15_GradioApp.py` (Gradio / Hugging Face)\n",
        "- `Group4_Module15_StreamlitApp.py` (Streamlit / Streamlit Cloud)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Team planning and role assignment\n",
        "\n",
        "**Team discussion: Decide how your team will divide the work and collaborate.**\n",
        "\n",
        "**Suggested roles (adapt as needed):**\n",
        "- **Gradio developer:** Lead rapid prototyping and Gradio interface development\n",
        "- **Streamlit architect:** Design and implement Streamlit dashboard structure\n",
        "- **Backend integrator:** Connect LLM, explainability, and feedback systems\n",
        "- **Deployment specialist:** Handle deployment, testing, and documentation\n",
        "\n",
        "**Your team's role distribution:**\n",
        "\n",
        "| Team Member | Primary Role | Secondary Responsibilities |\n",
        "|------------|--------------|----------------------------|\n",
        "| Member A | System Architect & LLM Integration Lead | Defines LLM workflow, memory strategy, prompt policy, error handling |\n",
        "| Member B | Explainability & Evaluation Lead (SHAP/LIME) | Designs quality signals, validates explainability outputs, test plans |\n",
        "| Member C | Gradio Prototype Developer | Implements Gradio UI, integrates explanation panel, local testing |\n",
        "| Member D | Streamlit Dashboard Architect | Implements Streamlit pages/state/caching, dashboard UX, local testing |\n",
        "| Member E | Deployment & Documentation Specialist | GitHub repo hygiene, HF/Streamlit deploy, secrets, README/notebook/URLs |\n",
        "\n",
        "**Application focus:** Tutor/Explainer chatbot on **Trustworthy AI**, aligned with prior modules (chat + memory + explainability + feedback).\n",
        "\n",
        "**Target users:** University students, teaching staff, and institutional support personnel seeking trustworthy AI guidance and evaluation patterns.\n",
        "\n",
        "**Collaboration approach:** GitHub (version control + deployments), WhatsApp (coordination), VS Code (local development & testing).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Environment setup\n",
        "\n",
        "Install required libraries for UI development, LLM integration, and explainability.\n",
        "\n",
        "**Note:** This notebook is for learning and documentation. Final deliverables are:\n",
        "1. `Group4_Module15_GradioApp.py` - Gradio prototype (HF Spaces)\n",
        "2. `Group4_Module15_StreamlitApp.py` - Streamlit dashboard (Streamlit Cloud)\n",
        "\n",
        "Templates were adapted, tested locally in VS Code, and deployed with secrets management.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required libraries (uncomment to install)\n",
        "# !pip install gradio streamlit\n",
        "# !pip install openai langchain langchain-openai langchain-community\n",
        "# !pip install pandas plotly\n",
        "# !pip install shap lime  # For explainability\n",
        "# !pip install sentence-transformers  # For embeddings\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import time\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "print(\"‚úÖ Libraries ready!\")\n",
        "print(\"\\nUI Frameworks:\")\n",
        "print(\"  - Gradio: For rapid prototyping\")\n",
        "print(\"  - Streamlit: For dashboards\")\n",
        "print(\"\\nLLM Options:\")\n",
        "print(\"  - OpenAI API\")\n",
        "print(\"  - AWS Bedrock\")\n",
        "print(\"  - Local Ollama\")\n",
        "print(\"  - Hugging Face Transformers\")\n",
        "print(\"\\nExplainability:\")\n",
        "print(\"  - SHAP (global)\")\n",
        "print(\"  - LIME (local)\")\n",
        "\n",
        "print(\"\\n Using OpenAI API via environment/secrets in deployed apps (HF + Streamlit Cloud)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Gradio rapid prototyping\n",
        "\n",
        "### Understanding Gradio\n",
        "\n",
        "**What is Gradio?**\n",
        "\n",
        "Gradio is a Python library for quickly creating web interfaces for your machine learning models and functions.\n",
        "\n",
        "**Key advantages:**\n",
        "- Wrap any Python function in a web UI (minutes, not hours)\n",
        "- Built-in components for ML tasks (chat, file upload, etc.)\n",
        "- Native integration with Hugging Face Spaces\n",
        "- Share via public link instantly\n",
        "- Perfect for demos and proof-of-concepts\n",
        "\n",
        "**Architecture pattern:**\n",
        "```\n",
        "Your Python Function ‚Üí gr.Interface(fn=...) ‚Üí Web Application\n",
        "```\n",
        "\n",
        "**When to use Gradio:**\n",
        "- Quick prototypes and demos\n",
        "- Sharing single functions/models\n",
        "- PoC for stakeholders\n",
        "- Hugging Face deployment\n",
        "\n",
        "**Team note:** We implemented the full Gradio prototype and deployed it to Hugging Face Spaces with secure secrets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradio Example 1: Simple Interface\n",
        "import gradio as gr\n",
        "\n",
        "def text_analyzer(text: str, analysis_type: str) -> dict:\n",
        "    \"\"\"Analyze text and return statistics.\"\"\"\n",
        "    return {\n",
        "        \"Character count\": len(text),\n",
        "        \"Word count\": len(text.split()),\n",
        "        \"Analysis type\": analysis_type,\n",
        "        \"Sample\": text[:50] + \"...\" if len(text) > 50 else text\n",
        "    }\n",
        "\n",
        "# Create Gradio interface\n",
        "demo_simple = gr.Interface(\n",
        "    fn=text_analyzer,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Enter text\", lines=5, placeholder=\"Type or paste text here...\"),\n",
        "        gr.Radio(choices=[\"Basic\", \"Advanced\"], label=\"Analysis Type\", value=\"Basic\")\n",
        "    ],\n",
        "    outputs=gr.JSON(label=\"Results\"),\n",
        "    title=\"Text Analyzer Demo\",\n",
        "    description=\"Simple Gradio interface demonstrating input/output mapping\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Simple Gradio interface created!\")\n",
        "print(\"To run: demo_simple.launch()\")\n",
        "print(\"\\nüí° Key insight: Gradio wraps your function with minimal code!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gradio chatbot interface\n",
        "\n",
        "**gr.ChatInterface** - Pre-built component for conversational AI\n",
        "\n",
        "**Key features:**\n",
        "- Automatically manages conversation history\n",
        "- Built-in retry, undo, clear buttons\n",
        "- Example prompts for users\n",
        "- Streaming support\n",
        "- Custom styling\n",
        "\n",
        "**Function signature:**\n",
        "```python\n",
        "def chat_fn(message: str, history: List[Tuple[str, str]]) -> str:\n",
        "    # message: current user message\n",
        "    # history: list of (user_msg, bot_msg) tuples\n",
        "    # return: bot response\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradio Example 2: Chatbot with Memory\n",
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "def chatbot_response(message: str, history: list) -> str:\n",
        "    \"\"\"\n",
        "    Generate chatbot response with context awareness.\n",
        "    \n",
        "    Args:\n",
        "        message: Current user message\n",
        "        history: List of (user_msg, bot_msg) tuples\n",
        "    \n",
        "    Returns:\n",
        "        Bot response string\n",
        "    \"\"\"\n",
        "    # Simulate processing time\n",
        "    time.sleep(0.5)\n",
        "    \n",
        "    # Context-aware response\n",
        "    num_exchanges = len(history)\n",
        "    \n",
        "    # Mock LLM response (replace with actual LLM)\n",
        "    if \"hello\" in message.lower():\n",
        "        response = f\"Hello! This is exchange #{num_exchanges + 1}.\"\n",
        "    elif \"history\" in message.lower():\n",
        "        response = f\"We've had {num_exchanges} exchanges so far.\"\n",
        "    elif num_exchanges > 0:\n",
        "        last_topic = history[-1][0][:30]\n",
        "        response = f\"You said: '{message}'. Earlier you asked about: '{last_topic}...'\"\n",
        "    else:\n",
        "        response = f\"You said: '{message}'. How can I help?\"\n",
        "    \n",
        "    return response\n",
        "\n",
        "# Create chatbot interface\n",
        "chat_demo = gr.ChatInterface(\n",
        "    fn=chatbot_response,\n",
        "    title=\"Chatbot with Memory Demo\",\n",
        "    description=\"Try asking about the conversation history!\",\n",
        "    examples=[\"Hello!\", \"What did I say before?\", \"Tell me about history\"],\n",
        "    retry_btn=\"üîÑ Retry\",\n",
        "    undo_btn=\"‚Ü©Ô∏è Undo\",  \n",
        "    clear_btn=\"üóëÔ∏è Clear History\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Gradio chatbot created!\")\n",
        "print(\"To run: chat_demo.launch()\")\n",
        "print(\"\\nüí° History is automatically managed by Gradio!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Streamlit dashboard development\n",
        "\n",
        "### Understanding Streamlit\n",
        "\n",
        "**What is Streamlit?**\n",
        "\n",
        "Streamlit turns Python scripts into interactive web applications.\n",
        "\n",
        "**Key differences from Gradio:**\n",
        "\n",
        "| Feature | Gradio | Streamlit |\n",
        "|---------|---------|-----------|\n",
        "| **Focus** | Function wrapping | Full applications |\n",
        "| **Complexity** | Simple demos | Complex dashboards |\n",
        "| **Layout** | Limited | Highly flexible |\n",
        "| **State** | Implicit | Explicit (`session_state`) |\n",
        "| **Best for** | Quick prototypes | Production apps |\n",
        "\n",
        "**Streamlit's reactive model:**\n",
        "1. User interacts (button, input, etc.)\n",
        "2. Entire script re-runs top-to-bottom\n",
        "3. UI updates with new values\n",
        "4. State persists via `st.session_state`\n",
        "\n",
        "**Critical concept: Caching**\n",
        "\n",
        "Since scripts re-run on every interaction, expensive operations must be cached:\n",
        "- `@st.cache_resource`: For models, connections (non-serialized objects)\n",
        "- `@st.cache_data`: For data loading, computations (serializable data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Streamlit Example: State Management Pattern\n",
        "# Note: This is pseudo-code for learning - see streamlit_app_template.py for working code\n",
        "\n",
        "streamlit_state_example = \"\"\"\n",
        "import streamlit as st\n",
        "\n",
        "# Initialize session state (runs once per session)\n",
        "if 'counter' not in st.session_state:\n",
        "    st.session_state.counter = 0\n",
        "\n",
        "if 'messages' not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display current state\n",
        "st.write(f\"Counter: {st.session_state.counter}\")\n",
        "st.write(f\"Messages: {len(st.session_state.messages)}\")\n",
        "\n",
        "# Buttons that modify state\n",
        "if st.button(\"Increment Counter\"):\n",
        "    st.session_state.counter += 1\n",
        "    st.rerun()  # Trigger re-run with new state\n",
        "\n",
        "# Text input\n",
        "if message := st.text_input(\"Add message\"):\n",
        "    st.session_state.messages.append(message)\n",
        "    # State automatically persists\n",
        "\n",
        "# Display all messages\n",
        "for i, msg in enumerate(st.session_state.messages):\n",
        "    st.write(f\"{i+1}. {msg}\")\n",
        "\"\"\"\n",
        "\n",
        "print(\"üìù Streamlit State Management Pattern:\")\n",
        "print(streamlit_state_example)\n",
        "print(\"\\n‚úÖ Key concepts:\")\n",
        "print(\"  - Initialize state on first run\")\n",
        "print(\"  - Access via st.session_state\")\n",
        "print(\"  - State persists across reruns\")\n",
        "print(\"  - Use st.rerun() when needed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Streamlit caching strategies\n",
        "\n",
        "**Why caching matters:**\n",
        "\n",
        "Without caching, expensive operations run on every user interaction:\n",
        "- Loading models (10+ seconds)\n",
        "- Computing embeddings (seconds per call)\n",
        "- API calls (cost + latency)\n",
        "- Database queries\n",
        "\n",
        "**Two caching decorators:**\n",
        "\n",
        "**@st.cache_resource** - For non-serializable objects\n",
        "```python\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    return HuggingFaceModel(\"model-name\")  # Loads once, reused forever\n",
        "```\n",
        "\n",
        "Use for:\n",
        "- ML models\n",
        "- Database connections\n",
        "- API clients\n",
        "- Thread pools\n",
        "\n",
        "**@st.cache_data** - For serializable data\n",
        "```python\n",
        "@st.cache_data(ttl=3600)  # Cache for 1 hour\n",
        "def load_data():\n",
        "    return pd.read_csv(\"data.csv\")  # Cached by input params\n",
        "```\n",
        "\n",
        "Use for:\n",
        "- DataFrames\n",
        "- Lists, dicts\n",
        "- Computation results\n",
        "- API responses (with TTL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Streamlit Caching Example\n",
        "caching_example = \"\"\"\n",
        "import streamlit as st\n",
        "import time\n",
        "\n",
        "# BAD: Without caching (runs every time)\n",
        "def expensive_computation_bad(x):\n",
        "    time.sleep(3)  # Simulates expensive operation\n",
        "    return x * 2\n",
        "\n",
        "# GOOD: With caching (runs once per unique input)\n",
        "@st.cache_data\n",
        "def expensive_computation_good(x):\n",
        "    time.sleep(3)  # Only runs first time for each x\n",
        "    return x * 2\n",
        "\n",
        "# Model loading example\n",
        "@st.cache_resource\n",
        "def load_llm():\n",
        "    # Loads once, cached forever\n",
        "    print(\"Loading model...\")  # Only prints once\n",
        "    return MockLLM()\n",
        "\n",
        "# Usage\n",
        "st.title(\"Caching Demo\")\n",
        "\n",
        "# First call: takes 3 seconds\n",
        "# Subsequent calls with same input: instant\n",
        "number = st.number_input(\"Enter number\", value=5)\n",
        "result = expensive_computation_good(number)\n",
        "st.write(f\"Result: {result}\")\n",
        "\n",
        "# Model loads once for entire session\n",
        "model = load_llm()\n",
        "\"\"\"\n",
        "\n",
        "print(\"üìù Streamlit Caching Example:\")\n",
        "print(caching_example)\n",
        "print(\"\\n‚úÖ Performance impact:\")\n",
        "print(\"  - Without caching: 3s per interaction\")\n",
        "print(\"  - With caching: 3s first time, <1ms after\")\n",
        "print(\"  - 3000x speedup!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Team project implementation\n",
        "\n",
        "### Your deliverables\n",
        "\n",
        "Your team will create two complete applications:\n",
        "\n",
        "**1. Gradio Prototype (`gradio_app_template.py`)**\n",
        "- Simple chatbot interface\n",
        "- LLM integration\n",
        "- Basic explainability display\n",
        "- User feedback mechanism (thumbs up/down)\n",
        "- Ready to deploy to Hugging Face Spaces\n",
        "\n",
        "**2. Streamlit Dashboard (`streamlit_app_template.py`)**\n",
        "- Multi-page application:\n",
        "  - Page 1: Chat interface with memory\n",
        "  - Page 2: Explainability analysis\n",
        "  - Page 3: Feedback dashboard\n",
        "  - Page 4: System monitoring\n",
        "  - Page 5: Documentation\n",
        "- State management for conversation history\n",
        "- Caching for expensive operations\n",
        "- Feedback collection and visualization\n",
        "- Performance metrics tracking\n",
        "\n",
        "### Implementation guidance\n",
        "\n",
        "**Templates provided:**\n",
        "\n",
        "Both application templates are provided in your project directory with:\n",
        "- Complete working code structure\n",
        "- Placeholder functions for LLM integration\n",
        "- Mock implementations for testing\n",
        "- Detailed comments and TODOs\n",
        "- Deployment instructions\n",
        "\n",
        "**Your team should:**\n",
        "\n",
        "1. **Customize the templates** for your use case\n",
        "2. **Integrate actual LLM** (OpenAI, Bedrock, Ollama, or Transformers)\n",
        "3. **Implement explainability** (SHAP or LIME visualizations)\n",
        "4. **Test thoroughly** with different inputs and edge cases\n",
        "5. **Deploy** to appropriate platform (Hugging Face Spaces or Streamlit Cloud)\n",
        "6. **Document** your customizations and decisions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Checking for deliverable files...\n",
            "\n",
            "‚úÖ ../apps/gradio/Group4_Module15_GradioApp.py (Found: 19,645 bytes)\n",
            "‚úÖ ../apps/streamlit/Group4_Module15_StreamlitApp.py (Found: 25,599 bytes)\n",
            "‚úÖ ../requirements.txt (Found: 515 bytes)\n",
            "‚úÖ ../README_Group4.md (Found: 7,213 bytes)\n",
            "‚úÖ ../deployment_urls.txt (Found: 316 bytes)\n",
            "\n",
            "üìù To run the applications locally:\n",
            "  Gradio:    python apps/gradio/Group4_Module15_GradioApp.py\n",
            "  Streamlit: streamlit run apps/streamlit/Group4_Module15_StreamlitApp.py\n",
            "\n",
            "üí° Deployed versions are available via deployment_urls.txt\n"
          ]
        }
      ],
      "source": [
        "# Verify template files exist\n",
        "# Verify deliverable app files exist (Group_4 structure)\n",
        "import os\n",
        "\n",
        "deliverable_files = [\n",
        "    \"../apps/gradio/Group4_Module15_GradioApp.py\",\n",
        "    \"../apps/streamlit/Group4_Module15_StreamlitApp.py\",\n",
        "    \"../requirements.txt\",\n",
        "    \"../README_Group4.md\",\n",
        "    \"../deployment_urls.txt\",\n",
        "]\n",
        "\n",
        "print(\"üîç Checking for deliverable files...\\n\")\n",
        "\n",
        "for file in deliverable_files:\n",
        "    if os.path.exists(file):\n",
        "        size = os.path.getsize(file)\n",
        "        print(f\"‚úÖ {file} (Found: {size:,} bytes)\")\n",
        "    else:\n",
        "        print(f\"‚ùå {file} NOT FOUND\")\n",
        "\n",
        "print(\"\\nüìù To run the applications locally:\")\n",
        "print(\"  Gradio:    python apps/gradio/Group4_Module15_GradioApp.py\")\n",
        "print(\"  Streamlit: streamlit run apps/streamlit/Group4_Module15_StreamlitApp.py\")\n",
        "print(\"\\nüí° Deployed versions are available via deployment_urls.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Deployment guide\n",
        "\n",
        "### Deployment options\n",
        "\n",
        "**1. Hugging Face Spaces (Gradio)**\n",
        "\n",
        "Best for: Quick demos, public sharing\n",
        "\n",
        "```bash\n",
        "# Setup\n",
        "git init\n",
        "echo \"gradio\\nopenai\" > requirements.txt\n",
        "git add gradio_app_template.py requirements.txt\n",
        "git commit -m \"Initial commit\"\n",
        "\n",
        "# Deploy\n",
        "# 1. Create new Space on huggingface.co\n",
        "# 2. Choose \"Gradio\" as SDK\n",
        "# 3. Push to the Space repo\n",
        "git remote add hf https://huggingface.co/spaces/YOUR_USERNAME/YOUR_SPACE\n",
        "git push hf main\n",
        "```\n",
        "\n",
        "**Important:** Use Hugging Face Secrets for API keys!\n",
        "\n",
        "**2. Streamlit Community Cloud**\n",
        "\n",
        "Best for: Team dashboards, internal tools\n",
        "\n",
        "```bash\n",
        "# Setup\n",
        "git init  \n",
        "echo \"streamlit\\npandas\\nplotly\" > requirements.txt\n",
        "git add streamlit_app_template.py requirements.txt\n",
        "git commit -m \"Initial commit\"\n",
        "git push origin main\n",
        "\n",
        "# Deploy\n",
        "# 1. Go to share.streamlit.io\n",
        "# 2. Connect your GitHub repo\n",
        "# 3. Select branch and file\n",
        "# 4. Deploy!\n",
        "```\n",
        "\n",
        "**Important:** Add secrets in Streamlit Cloud settings!\n",
        "\n",
        "**3. Local Docker**\n",
        "\n",
        "Best for: Development, self-hosted\n",
        "\n",
        "```dockerfile\n",
        "# Dockerfile for Streamlit\n",
        "FROM python:3.11-slim\n",
        "WORKDIR /app\n",
        "COPY requirements.txt .\n",
        "RUN pip install -r requirements.txt\n",
        "COPY streamlit_app_template.py .\n",
        "EXPOSE 8501\n",
        "CMD [\"streamlit\", \"run\", \"streamlit_app_template.py\"]\n",
        "```\n",
        "\n",
        "```bash\n",
        "docker build -t my-ai-dashboard .\n",
        "docker run -p 8501:8501 my-ai-dashboard\n",
        "```\n",
        "\n",
        "### Security best practices\n",
        "\n",
        "**Never hardcode API keys:**\n",
        "\n",
        "```python\n",
        "# WRONG\n",
        "openai.api_key = \"sk-abc123...\"\n",
        "\n",
        "# RIGHT - Environment variables\n",
        "import os\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# RIGHT - Streamlit secrets\n",
        "import streamlit as st\n",
        "openai.api_key = st.secrets[\"OPENAI_API_KEY\"]\n",
        "```\n",
        "\n",
        "**Secrets configuration:**\n",
        "\n",
        "**Hugging Face Spaces:**\n",
        "- Settings ‚Üí Repository secrets\n",
        "- Add `OPENAI_API_KEY = your_key`\n",
        "- Access via `os.getenv()`\n",
        "\n",
        "**Streamlit Cloud:**\n",
        "- App settings ‚Üí Secrets\n",
        "- Add TOML format:\n",
        "```toml\n",
        "OPENAI_API_KEY = \"your_key\"\n",
        "```\n",
        "- Access via `st.secrets[\"OPENAI_API_KEY\"]`\n",
        "\n",
        "\n",
        "**Team deployment used:** Hugging Face Spaces (Gradio) + Streamlit Community Cloud (Streamlit) + GitHub for source control.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Team project summary\n",
        "\n",
        "### A. Team information\n",
        "\n",
        "**Team Name:** Group_4\n",
        "\n",
        "**Application Name:** Trustworthy AI Tutor/Explainer Dashboard\n",
        "\n",
        "**Target Use Case:** Interactive LLM-based tutor/explainer for Trustworthy AI concepts with built-in explainability (SHAP/LIME), feedback logging, and basic monitoring.\n",
        "\n",
        "**Target Users:** University students, teaching staff, and institutional support personnel.\n",
        "\n",
        "### B. Gradio prototype summary\n",
        "\n",
        "**Completed features:**\n",
        "- [x] Basic interface with inputs/outputs\n",
        "- [x] LLM integration\n",
        "- [x] Chatbot with conversation history\n",
        "- [x] Explainability display\n",
        "- [x] User feedback mechanism\n",
        "- [x] Deployed to Hugging Face Spaces\n",
        "\n",
        "**LLM choice:** OpenAI API (OpenAI Python SDK)\n",
        "\n",
        "**Key customizations made:** Chat with memory, integrated Explainability panel (quality score + SHAP + LIME), user feedback logging to CSV, safe secrets handling (no hardcoded keys), mock fallback when key missing.\n",
        "\n",
        "**Deployment URL:** https://huggingface.co/spaces/alej-hugg/group-4-module15-gradio\n",
        "\n",
        "### C. Streamlit dashboard summary\n",
        "\n",
        "**Completed features:**\n",
        "- [x] Multi-page structure\n",
        "- [x] Chat interface with memory\n",
        "- [x] State management for history\n",
        "- [x] Caching for expensive operations\n",
        "- [x] Explainability analysis page\n",
        "- [x] Feedback collection and visualization\n",
        "- [x] Performance monitoring\n",
        "- [x] Documentation page\n",
        "- [x] Deployed to Streamlit Cloud\n",
        "\n",
        "**Pages implemented:**\n",
        "1. Chat (LLM + memory)\n",
        "2. Explainability (quality signals + SHAP/LIME summary)\n",
        "3. Feedback (CSV logging + basic review)\n",
        "4. Monitoring (simple runtime signals / counters)\n",
        "5. About/Docs (how-to + limitations)\n",
        "\n",
        "**State management approach:** `st.session_state` for conversation history and UI state; Gradio uses component state/history and message conversion utilities. No cross-user leakage (per-session state).\n",
        "\n",
        "**Caching strategy:** `@st.cache_resource` for the OpenAI client / model-like resources; lightweight computations cached where appropriate to reduce re-run overhead.\n",
        "\n",
        "**Deployment URL:** https://group-4-module15-strmlit.streamlit.app/\n",
        "\n",
        "### D. Explainability integration\n",
        "\n",
        "**Method used:** [x] SHAP [x] LIME [x] Both\n",
        "\n",
        "**Implementation details:** We compute a lightweight *quality score (0‚Äì1)* from observable output signals (e.g., presence of steps/structure, uncertainty markers, length control) and then use SHAP to explain which signals contributed most to the score. We also use LIME locally to highlight which user-input tokens are most influential for the produced response in that specific interaction.\n",
        "\n",
        "**Visualizations created:** Explainability summary (quality score + signals), SHAP top feature contributions table, LIME influential user tokens list.\n",
        "\n",
        "**User value provided:** Users can understand ‚Äúwhy‚Äù an answer is scored as higher quality and what parts of their prompt are steering the response, supporting transparency and iterative prompt improvement.\n",
        "\n",
        "### E. Feedback mechanism\n",
        "\n",
        "**Feedback type:** Thumbs up/down (and optional flags), captured per response.\n",
        "\n",
        "**Storage approach:** CSV logging inside the app runtime (prototype). In production, migrate to persistent DB/storage.\n",
        "\n",
        "**Analytics implemented:** Basic feedback counts and review via dashboard page; logs available for manual inspection and future aggregation.\n",
        "\n",
        "**Total feedback collected (during testing):** 5‚Äì15 (during structured test prompts; may vary by run)\n",
        "\n",
        "### F. Team collaboration\n",
        "\n",
        "**Role distribution:**\n",
        "\n",
        "| Team Member | Primary Role | Key Contributions |\n",
        "|------------|--------------|-------------------|\n",
        "| Member A | System Architect & LLM Integration Lead | LLM workflow, memory strategy, prompt policy, error handling |\n",
        "| Member B | Explainability & Evaluation Lead (SHAP/LIME) | Quality signals, SHAP/LIME integration, testing protocol |\n",
        "| Member C | Gradio Prototype Developer | Gradio UI, explainability panel, local tests, HF readiness |\n",
        "| Member D | Streamlit Dashboard Architect | Pages/state/caching, UX, local tests, Cloud readiness |\n",
        "| Member E | Deployment & Documentation Specialist | GitHub, secrets, deployments, README, URLs, notebook completion |\n",
        "\n",
        "**Collaboration tools used:** GitHub, WhatsApp, VS Code.\n",
        "\n",
        "**Challenges encountered:** Secrets/env loading differences between local, HF Spaces, and Streamlit Cloud; Gradio message formatting differences across versions; stable state handling across reruns in Streamlit.\n",
        "\n",
        "**Solutions implemented:** Platform-specific secrets usage (HF Secrets / Streamlit Secrets), robust fallback to mock mode, history normalization utilities for Gradio, session_state patterns and caching for Streamlit.\n",
        "\n",
        "### G. Technical decisions\n",
        "\n",
        "**Framework comparison insights:**\n",
        "- Gradio is optimal for fast demos and single-purpose UIs; Streamlit is optimal for structured dashboards and multi-page applications.\n",
        "\n",
        "**When to use Gradio:**\n",
        "- Rapid prototyping and demos (minutes to UI)\n",
        "- Hugging Face Spaces deployment for public sharing\n",
        "\n",
        "**When to use Streamlit:**\n",
        "- Multi-page dashboards with monitoring/feedback views\n",
        "- Fine-grained layout control, state and caching for ‚Äúapp-like‚Äù behavior\n",
        "\n",
        "**Most valuable feature learned:** Managing state + explainability side-by-side with LLM outputs (transparent UX).\n",
        "\n",
        "**Most challenging aspect:** Ensuring consistent secrets/config behavior across deployment platforms while keeping the code secure.\n",
        "\n",
        "### H. Deployment experience\n",
        "\n",
        "**Platform(s) used:** Hugging Face Spaces (Gradio) + Streamlit Community Cloud (Streamlit)\n",
        "\n",
        "**Deployment challenges:** Correct file paths, secrets configuration, and ensuring the app runs outside local `.env` environments.\n",
        "\n",
        "**Security measures implemented:** No hardcoded secrets; API keys stored only in HF Secrets and Streamlit Secrets; `.env` kept local and excluded from Git tracking.\n",
        "\n",
        "**Performance observations:** Streamlit caching improved responsiveness by avoiding reinitialization on every interaction; overall latency dominated by LLM API calls.\n",
        "\n",
        "### I. Future improvements\n",
        "\n",
        "**Short-term enhancements:**\n",
        "1. Replace CSV with persistent storage (SQLite/Postgres) for feedback\n",
        "2. Add standardized evaluation metrics (rubric-based scoring, safety checks)\n",
        "3. Add better observability (structured logs, tracing, error dashboards)\n",
        "\n",
        "**Long-term vision:**\n",
        "A production-grade trustworthy tutoring platform with robust evaluation, auditing, and monitoring for institutional use.\n",
        "\n",
        "**Scalability considerations:**\n",
        "Rate limiting, caching, batching, persistent storage, and separation of UI from backend services.\n",
        "\n",
        "### J. Learning outcomes\n",
        "\n",
        "**Key concepts mastered:**\n",
        "- [x] Gradio interface development\n",
        "- [x] Streamlit dashboard creation\n",
        "- [x] State management in reactive apps\n",
        "- [x] Caching strategies for performance\n",
        "- [x] Multi-page application architecture\n",
        "- [x] Deployment to cloud platforms\n",
        "- [x] API key security best practices\n",
        "- [x] User feedback collection\n",
        "- [x] Explainability integration\n",
        "\n",
        "**Most valuable insight:** Trustworthy AI UX is not only model choice‚Äîit's also explainability, feedback loops, and deployable interfaces with secure configuration.\n",
        "\n",
        "**How this applies to real projects:** The same pattern generalizes to institutional chatbots (policies, student support, compliance) where transparency, evaluation, and monitoring are mandatory.\n",
        "\n",
        "**Skills gained:** UI prototyping (Gradio), dashboard engineering (Streamlit), secrets-based deployments, and practical explainability integration for LLM applications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Team submission checklist\n",
        "\n",
        "Before submitting, ensure your team has completed:\n",
        "\n",
        "**Planning and setup:**\n",
        "- [X] Team roles clearly defined and documented\n",
        "- [X] Application use case and target users identified\n",
        "- [X] Development environment configured\n",
        "- [X] All required libraries installed\n",
        "\n",
        "**Gradio prototype:**\n",
        "- [X] `gradio_app_template.py` customized for your use case\n",
        "- [X] LLM integrated (OpenAI/Bedrock/Ollama/Transformers)\n",
        "- [X] Chatbot interface functional with conversation memory\n",
        "- [X] Explainability display implemented\n",
        "- [X] User feedback mechanism (thumbs up/down) working\n",
        "- [X] Tested with multiple example conversations\n",
        "- [X] Deployed to Hugging Face Spaces (or local deployment documented)\n",
        "- [X] API keys properly secured (no hardcoded secrets)\n",
        "\n",
        "**Streamlit dashboard:**\n",
        "- [X] `streamlit_app_template.py` customized for your use case\n",
        "- [X] Multi-page structure implemented\n",
        "- [X] Chat page with conversation history working\n",
        "- [X] State management implemented for persistence\n",
        "- [X] Caching applied to expensive operations\n",
        "- [X] Explainability analysis page functional\n",
        "- [X] Feedback dashboard with visualizations\n",
        "- [X] Monitoring page showing metrics\n",
        "- [X] Documentation page completed with team information\n",
        "- [X] Tested thoroughly (state, caching, all pages)\n",
        "- [X] Deployed to Streamlit Cloud (or local deployment documented)\n",
        "- [X] API keys secured via secrets management\n",
        "\n",
        "**Explainability integration:**\n",
        "- [X] SHAP or LIME implemented\n",
        "- [X] Visualizations display correctly\n",
        "- [X] Explanations integrated with LLM responses\n",
        "- [X] Users can understand model decisions\n",
        "\n",
        "**Documentation:**\n",
        "- [X] Team information documented\n",
        "- [X] Role distribution explained\n",
        "- [X] Technical decisions justified\n",
        "- [X] Deployment instructions provided\n",
        "- [X] User guide included\n",
        "- [X] Code well-commented\n",
        "\n",
        "**Testing:**\n",
        "- [X] Both applications tested with various inputs\n",
        "- [X] Edge cases handled (empty input, long text, errors)\n",
        "- [X] Performance acceptable (caching working)\n",
        "- [X] Feedback mechanism collecting data properly\n",
        "- [X] Deployment working (accessible URLs)\n",
        "\n",
        "**Submission files:**\n",
        "- [X] `Group4_Module15_Project.ipynb` (this completed notebook)\n",
        "- [X] `Group4_Module15_GradioApp.py` (customized Gradio app)\n",
        "- [X] `Group4_Module15_StreamlitApp.py` (customized Streamlit app)\n",
        "- [X] `requirements.txt` (all dependencies listed)\n",
        "- [X] `README_Group4.md` (deployment and usage instructions)\n",
        "- [X] `deployment_urls.txt` (links to deployed applications)\n",
        "\n",
        "**Estimated completion time: 80 minutes (per team member)**\n",
        "\n",
        "---\n",
        "\n",
        "## Resources and next steps\n",
        "\n",
        "**Official documentation:**\n",
        "- Gradio: https://www.gradio.app/docs\n",
        "- Streamlit: https://docs.streamlit.io/\n",
        "- Hugging Face Spaces: https://huggingface.co/docs/hub/spaces\n",
        "- Streamlit Community Cloud: https://streamlit.io/cloud\n",
        "\n",
        "**Deployment platforms:**\n",
        "- Hugging Face Spaces: https://huggingface.co/spaces\n",
        "- Streamlit Cloud: https://share.streamlit.io\n",
        "- Docker Hub: https://hub.docker.com/\n",
        "\n",
        "**Explainability libraries:**\n",
        "- SHAP: https://shap.readthedocs.io/\n",
        "- LIME: https://lime-ml.readthedocs.io/\n",
        "\n",
        "**LLM integration:**\n",
        "- OpenAI: https://platform.openai.com/docs\n",
        "- LangChain: https://python.langchain.com/docs\n",
        "- Hugging Face Transformers: https://huggingface.co/docs/transformers\n",
        "\n",
        "---\n",
        "\n",
        "**End of Module 15 Team Project**\n",
        "\n",
        "**Good luck with your Trustworthy AI Explainer Dashboard!** \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mod15_env (3.12.4)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
