{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trustworthy AI Explainer Dashboard: Interactive UI for LLM Applications\n",
    "**Module 15 Team Project**\n",
    "\n",
    "---\n",
    "\n",
    "**Team Name:** Group4\n",
    "\n",
    "**Team Members:**\n",
    "1. Member A (PM/Integrator)\n",
    "2. Member B (Gradio)\n",
    "3. Member C (Streamlit)\n",
    "4. Member D (Explainability)\n",
    "5. Member E (Deployment/Docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team planning and role assignment\n",
    "\n",
    "**Team discussion: Decide how your team will divide the work and collaborate.**\n",
    "\n",
    "**Suggested roles (adapt as needed):**\n",
    "- **Gradio developer:** Lead rapid prototyping and Gradio interface development\n",
    "- **Streamlit architect:** Design and implement Streamlit dashboard structure\n",
    "- **Backend integrator:** Connect LLM, explainability, and feedback systems\n",
    "- **Deployment specialist:** Handle deployment, testing, and documentation\n",
    "\n",
    "**Your team's role distribution:**\n",
    "\n",
    "| Team Member | Primary Role | Secondary Responsibilities |\n",
    "|------------|--------------|----------------------------|\n",
    "| | | |\n",
    "| | | |\n",
    "| | | |\n",
    "| | | |\n",
    "\n",
    "**Application focus:** ___________________________\n",
    "\n",
    "**Target users:** ___________________________\n",
    "\n",
    "**Collaboration approach:** ___________________________\n",
    "\n",
    "**Chosen use case (simple, no RAG):** Tutor/Explainer chatbot for Trustworthy AI concepts (memory + explainability + feedback).\n",
    "\n",
    "**Explainability plan:** LIME (local token influence on input) + SHAP (tabular feature contributions for a lightweight quality score).\n",
    "\n",
    "**Deployment plan:** Gradio ‚Üí Hugging Face Spaces, Streamlit ‚Üí Streamlit Community Cloud.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment setup\n",
    "\n",
    "Install required libraries for UI development, LLM integration, and explainability.\n",
    "\n",
    "**Note:** This notebook is for learning and prototyping. Your final deliverables will be:\n",
    "1. `gradio_app_template.py` - Gradio prototype\n",
    "2. `streamlit_app_template.py` - Streamlit dashboard\n",
    "\n",
    "These template files are provided in your project directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (uncomment to install)\n",
    "# !pip install gradio streamlit\n",
    "# !pip install openai langchain langchain-openai langchain-community\n",
    "# !pip install pandas plotly\n",
    "# !pip install shap lime  # For explainability\n",
    "# !pip install sentence-transformers  # For embeddings\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "print(\"‚úÖ Libraries ready!\")\n",
    "print(\"\\nUI Frameworks:\")\n",
    "print(\"  - Gradio: For rapid prototyping\")\n",
    "print(\"  - Streamlit: For dashboards\")\n",
    "print(\"\\nLLM Options:\")\n",
    "print(\"  - OpenAI API\")\n",
    "print(\"  - AWS Bedrock\")\n",
    "print(\"  - Local Ollama\")\n",
    "print(\"  - Hugging Face Transformers\")\n",
    "print(\"\\nExplainability:\")\n",
    "print(\"  - SHAP (global)\")\n",
    "print(\"  - LIME (local)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Gradio rapid prototyping\n",
    "\n",
    "### Understanding Gradio\n",
    "\n",
    "**What is Gradio?**\n",
    "\n",
    "Gradio is a Python library for quickly creating web interfaces for your machine learning models and functions.\n",
    "\n",
    "**Key advantages:**\n",
    "- Wrap any Python function in a web UI (minutes, not hours)\n",
    "- Built-in components for ML tasks (chat, file upload, etc.)\n",
    "- Native integration with Hugging Face Spaces\n",
    "- Share via public link instantly\n",
    "- Perfect for demos and proof-of-concepts\n",
    "\n",
    "**Architecture pattern:**\n",
    "```\n",
    "Your Python Function ‚Üí gr.Interface(fn=...) ‚Üí Web Application\n",
    "```\n",
    "\n",
    "**When to use Gradio:**\n",
    "- Quick prototypes and demos\n",
    "- Sharing single functions/models\n",
    "- PoC for stakeholders\n",
    "- Hugging Face deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio Example 1: Simple Interface\n",
    "import gradio as gr\n",
    "\n",
    "def text_analyzer(text: str, analysis_type: str) -> dict:\n",
    "    \"\"\"Analyze text and return statistics.\"\"\"\n",
    "    return {\n",
    "        \"Character count\": len(text),\n",
    "        \"Word count\": len(text.split()),\n",
    "        \"Analysis type\": analysis_type,\n",
    "        \"Sample\": text[:50] + \"...\" if len(text) > 50 else text\n",
    "    }\n",
    "\n",
    "# Create Gradio interface\n",
    "demo_simple = gr.Interface(\n",
    "    fn=text_analyzer,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Enter text\", lines=5, placeholder=\"Type or paste text here...\"),\n",
    "        gr.Radio(choices=[\"Basic\", \"Advanced\"], label=\"Analysis Type\", value=\"Basic\")\n",
    "    ],\n",
    "    outputs=gr.JSON(label=\"Results\"),\n",
    "    title=\"Text Analyzer Demo\",\n",
    "    description=\"Simple Gradio interface demonstrating input/output mapping\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Simple Gradio interface created!\")\n",
    "print(\"To run: demo_simple.launch()\")\n",
    "print(\"\\nüí° Key insight: Gradio wraps your function with minimal code!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio chatbot interface\n",
    "\n",
    "**gr.ChatInterface** - Pre-built component for conversational AI\n",
    "\n",
    "**Key features:**\n",
    "- Automatically manages conversation history\n",
    "- Built-in retry, undo, clear buttons\n",
    "- Example prompts for users\n",
    "- Streaming support\n",
    "- Custom styling\n",
    "\n",
    "**Function signature:**\n",
    "```python\n",
    "def chat_fn(message: str, history: List[Tuple[str, str]]) -> str:\n",
    "    # message: current user message\n",
    "    # history: list of (user_msg, bot_msg) tuples\n",
    "    # return: bot response\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio Example 2: Chatbot with Memory\n",
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "def chatbot_response(message: str, history: list) -> str:\n",
    "    \"\"\"\n",
    "    Generate chatbot response with context awareness.\n",
    "    \n",
    "    Args:\n",
    "        message: Current user message\n",
    "        history: List of (user_msg, bot_msg) tuples\n",
    "    \n",
    "    Returns:\n",
    "        Bot response string\n",
    "    \"\"\"\n",
    "    # Simulate processing time\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    # Context-aware response\n",
    "    num_exchanges = len(history)\n",
    "    \n",
    "    # Mock LLM response (replace with actual LLM)\n",
    "    if \"hello\" in message.lower():\n",
    "        response = f\"Hello! This is exchange #{num_exchanges + 1}.\"\n",
    "    elif \"history\" in message.lower():\n",
    "        response = f\"We've had {num_exchanges} exchanges so far.\"\n",
    "    elif num_exchanges > 0:\n",
    "        last_topic = history[-1][0][:30]\n",
    "        response = f\"You said: '{message}'. Earlier you asked about: '{last_topic}...'\"\n",
    "    else:\n",
    "        response = f\"You said: '{message}'. How can I help?\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Create chatbot interface\n",
    "chat_demo = gr.ChatInterface(\n",
    "    fn=chatbot_response,\n",
    "    title=\"Chatbot with Memory Demo\",\n",
    "    description=\"Try asking about the conversation history!\",\n",
    "    examples=[\"Hello!\", \"What did I say before?\", \"Tell me about history\"],\n",
    "    retry_btn=\"üîÑ Retry\",\n",
    "    undo_btn=\"‚Ü©Ô∏è Undo\",  \n",
    "    clear_btn=\"üóëÔ∏è Clear History\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Gradio chatbot created!\")\n",
    "print(\"To run: chat_demo.launch()\")\n",
    "print(\"\\nüí° History is automatically managed by Gradio!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Streamlit dashboard development\n",
    "\n",
    "### Understanding Streamlit\n",
    "\n",
    "**What is Streamlit?**\n",
    "\n",
    "Streamlit turns Python scripts into interactive web applications.\n",
    "\n",
    "**Key differences from Gradio:**\n",
    "\n",
    "| Feature | Gradio | Streamlit |\n",
    "|---------|---------|-----------|\n",
    "| **Focus** | Function wrapping | Full applications |\n",
    "| **Complexity** | Simple demos | Complex dashboards |\n",
    "| **Layout** | Limited | Highly flexible |\n",
    "| **State** | Implicit | Explicit (`session_state`) |\n",
    "| **Best for** | Quick prototypes | Production apps |\n",
    "\n",
    "**Streamlit's reactive model:**\n",
    "1. User interacts (button, input, etc.)\n",
    "2. Entire script re-runs top-to-bottom\n",
    "3. UI updates with new values\n",
    "4. State persists via `st.session_state`\n",
    "\n",
    "**Critical concept: Caching**\n",
    "\n",
    "Since scripts re-run on every interaction, expensive operations must be cached:\n",
    "- `@st.cache_resource`: For models, connections (non-serialized objects)\n",
    "- `@st.cache_data`: For data loading, computations (serializable data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlit Example: State Management Pattern\n",
    "# Note: This is pseudo-code for learning - see streamlit_app_template.py for working code\n",
    "\n",
    "streamlit_state_example = \"\"\"\n",
    "import streamlit as st\n",
    "\n",
    "# Initialize session state (runs once per session)\n",
    "if 'counter' not in st.session_state:\n",
    "    st.session_state.counter = 0\n",
    "\n",
    "if 'messages' not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Display current state\n",
    "st.write(f\"Counter: {st.session_state.counter}\")\n",
    "st.write(f\"Messages: {len(st.session_state.messages)}\")\n",
    "\n",
    "# Buttons that modify state\n",
    "if st.button(\"Increment Counter\"):\n",
    "    st.session_state.counter += 1\n",
    "    st.rerun()  # Trigger re-run with new state\n",
    "\n",
    "# Text input\n",
    "if message := st.text_input(\"Add message\"):\n",
    "    st.session_state.messages.append(message)\n",
    "    # State automatically persists\n",
    "\n",
    "# Display all messages\n",
    "for i, msg in enumerate(st.session_state.messages):\n",
    "    st.write(f\"{i+1}. {msg}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìù Streamlit State Management Pattern:\")\n",
    "print(streamlit_state_example)\n",
    "print(\"\\n‚úÖ Key concepts:\")\n",
    "print(\"  - Initialize state on first run\")\n",
    "print(\"  - Access via st.session_state\")\n",
    "print(\"  - State persists across reruns\")\n",
    "print(\"  - Use st.rerun() when needed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streamlit caching strategies\n",
    "\n",
    "**Why caching matters:**\n",
    "\n",
    "Without caching, expensive operations run on every user interaction:\n",
    "- Loading models (10+ seconds)\n",
    "- Computing embeddings (seconds per call)\n",
    "- API calls (cost + latency)\n",
    "- Database queries\n",
    "\n",
    "**Two caching decorators:**\n",
    "\n",
    "**@st.cache_resource** - For non-serializable objects\n",
    "```python\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    return HuggingFaceModel(\"model-name\")  # Loads once, reused forever\n",
    "```\n",
    "\n",
    "Use for:\n",
    "- ML models\n",
    "- Database connections\n",
    "- API clients\n",
    "- Thread pools\n",
    "\n",
    "**@st.cache_data** - For serializable data\n",
    "```python\n",
    "@st.cache_data(ttl=3600)  # Cache for 1 hour\n",
    "def load_data():\n",
    "    return pd.read_csv(\"data.csv\")  # Cached by input params\n",
    "```\n",
    "\n",
    "Use for:\n",
    "- DataFrames\n",
    "- Lists, dicts\n",
    "- Computation results\n",
    "- API responses (with TTL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlit Caching Example\n",
    "caching_example = \"\"\"\n",
    "import streamlit as st\n",
    "import time\n",
    "\n",
    "# BAD: Without caching (runs every time)\n",
    "def expensive_computation_bad(x):\n",
    "    time.sleep(3)  # Simulates expensive operation\n",
    "    return x * 2\n",
    "\n",
    "# GOOD: With caching (runs once per unique input)\n",
    "@st.cache_data\n",
    "def expensive_computation_good(x):\n",
    "    time.sleep(3)  # Only runs first time for each x\n",
    "    return x * 2\n",
    "\n",
    "# Model loading example\n",
    "@st.cache_resource\n",
    "def load_llm():\n",
    "    # Loads once, cached forever\n",
    "    print(\"Loading model...\")  # Only prints once\n",
    "    return MockLLM()\n",
    "\n",
    "# Usage\n",
    "st.title(\"Caching Demo\")\n",
    "\n",
    "# First call: takes 3 seconds\n",
    "# Subsequent calls with same input: instant\n",
    "number = st.number_input(\"Enter number\", value=5)\n",
    "result = expensive_computation_good(number)\n",
    "st.write(f\"Result: {result}\")\n",
    "\n",
    "# Model loads once for entire session\n",
    "model = load_llm()\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìù Streamlit Caching Example:\")\n",
    "print(caching_example)\n",
    "print(\"\\n‚úÖ Performance impact:\")\n",
    "print(\"  - Without caching: 3s per interaction\")\n",
    "print(\"  - With caching: 3s first time, <1ms after\")\n",
    "print(\"  - 3000x speedup!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Team project implementation\n",
    "\n",
    "### Your deliverables\n",
    "\n",
    "Your team will create two complete applications:\n",
    "\n",
    "**1. Gradio Prototype (`Group4_Module15_GradioApp.py`)**\n",
    "- Simple chatbot interface\n",
    "- LLM integration\n",
    "- Basic explainability display\n",
    "- User feedback mechanism (thumbs up/down)\n",
    "- Ready to deploy to Hugging Face Spaces\n",
    "\n",
    "**2. Streamlit Dashboard (`Group4_Module15_StreamlitApp.py`)**\n",
    "- Multi-page application:\n",
    "  - Page 1: Chat interface with memory\n",
    "  - Page 2: Explainability analysis\n",
    "  - Page 3: Feedback dashboard\n",
    "  - Page 4: System monitoring\n",
    "  - Page 5: Documentation\n",
    "- State management for conversation history\n",
    "- Caching for expensive operations\n",
    "- Feedback collection and visualization\n",
    "- Performance metrics tracking\n",
    "\n",
    "### Implementation guidance\n",
    "\n",
    "**Templates provided:**\n",
    "\n",
    "Both application templates are provided in your project directory with:\n",
    "- Complete working code structure\n",
    "- Placeholder functions for LLM integration\n",
    "- Mock implementations for testing\n",
    "- Detailed comments and TODOs\n",
    "- Deployment instructions\n",
    "\n",
    "**Your team should:**\n",
    "\n",
    "1. **Customize the templates** for your use case\n",
    "2. **Integrate actual LLM** (OpenAI, Bedrock, Ollama, or Transformers)\n",
    "3. **Implement explainability** (SHAP or LIME visualizations)\n",
    "4. **Test thoroughly** with different inputs and edge cases\n",
    "5. **Deploy** to appropriate platform (Hugging Face Spaces or Streamlit Cloud)\n",
    "6. **Document** your customizations and decisions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify template files exist\n",
    "import os\n",
    "\n",
    "template_files = [\n",
    "    \"Group4_Module15_GradioApp.py\",\n",
    "    \"Group4_Module15_StreamlitApp.py\"\n",
    "]\n",
    "\n",
    "print(\"üîç Checking for template files...\\n\")\n",
    "\n",
    "for file in template_files:\n",
    "    if os.path.exists(file):\n",
    "        size = os.path.getsize(file)\n",
    "        print(f\"‚úÖ {file} (Found: {size:,} bytes)\")\n",
    "    else:\n",
    "        print(f\"‚ùå {file} NOT FOUND\")\n",
    "\n",
    "print(\"\\nüìù To run the applications:\")\n",
    "print(\"  Gradio:    python Group4_Module15_GradioApp.py\")\n",
    "print(\"  Streamlit: streamlit run Group4_Module15_StreamlitApp.py\")\n",
    "print(\"\\nüí° Both templates are fully functional with mock LLM!\")\n",
    "print(\"   Customize them for your team's project.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Deployment guide\n",
    "\n",
    "### Deployment options\n",
    "\n",
    "**1. Hugging Face Spaces (Gradio)**\n",
    "\n",
    "Best for: Quick demos, public sharing\n",
    "\n",
    "```bash\n",
    "# Setup\n",
    "git init\n",
    "echo \"gradio\\nopenai\" > requirements.txt\n",
    "git add gradio_app_template.py requirements.txt\n",
    "git commit -m \"Initial commit\"\n",
    "\n",
    "# Deploy\n",
    "# 1. Create new Space on huggingface.co\n",
    "# 2. Choose \"Gradio\" as SDK\n",
    "# 3. Push to the Space repo\n",
    "git remote add hf https://huggingface.co/spaces/YOUR_USERNAME/YOUR_SPACE\n",
    "git push hf main\n",
    "```\n",
    "\n",
    "**Important:** Use Hugging Face Secrets for API keys!\n",
    "\n",
    "**2. Streamlit Community Cloud**\n",
    "\n",
    "Best for: Team dashboards, internal tools\n",
    "\n",
    "```bash\n",
    "# Setup\n",
    "git init  \n",
    "echo \"streamlit\\npandas\\nplotly\" > requirements.txt\n",
    "git add streamlit_app_template.py requirements.txt\n",
    "git commit -m \"Initial commit\"\n",
    "git push origin main\n",
    "\n",
    "# Deploy\n",
    "# 1. Go to share.streamlit.io\n",
    "# 2. Connect your GitHub repo\n",
    "# 3. Select branch and file\n",
    "# 4. Deploy!\n",
    "```\n",
    "\n",
    "**Important:** Add secrets in Streamlit Cloud settings!\n",
    "\n",
    "**3. Local Docker**\n",
    "\n",
    "Best for: Development, self-hosted\n",
    "\n",
    "```dockerfile\n",
    "# Dockerfile for Streamlit\n",
    "FROM python:3.11-slim\n",
    "WORKDIR /app\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt\n",
    "COPY streamlit_app_template.py .\n",
    "EXPOSE 8501\n",
    "CMD [\"streamlit\", \"run\", \"streamlit_app_template.py\"]\n",
    "```\n",
    "\n",
    "```bash\n",
    "docker build -t my-ai-dashboard .\n",
    "docker run -p 8501:8501 my-ai-dashboard\n",
    "```\n",
    "\n",
    "### Security best practices\n",
    "\n",
    "**Never hardcode API keys:**\n",
    "\n",
    "```python\n",
    "# WRONG\n",
    "openai.api_key = \"sk-abc123...\"\n",
    "\n",
    "# RIGHT - Environment variables\n",
    "import os\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# RIGHT - Streamlit secrets\n",
    "import streamlit as st\n",
    "openai.api_key = st.secrets[\"OPENAI_API_KEY\"]\n",
    "```\n",
    "\n",
    "**Secrets configuration:**\n",
    "\n",
    "**Hugging Face Spaces:**\n",
    "- Settings ‚Üí Repository secrets\n",
    "- Add `OPENAI_API_KEY = your_key`\n",
    "- Access via `os.getenv()`\n",
    "\n",
    "**Streamlit Cloud:**\n",
    "- App settings ‚Üí Secrets\n",
    "- Add TOML format:\n",
    "```toml\n",
    "OPENAI_API_KEY = \"your_key\"\n",
    "```\n",
    "- Access via `st.secrets[\"OPENAI_API_KEY\"]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team project summary\n",
    "\n",
    "### A. Team information\n",
    "\n",
    "**Team Name:** Group4\n",
    "\n",
    "**Application Name:** Trustworthy AI Tutor (Explainable)\n",
    "\n",
    "**Target Use Case:** Tutor/Explainer chatbot for Trustworthy AI (no RAG) with memory, LIME/SHAP, feedback and monitoring.\n",
    "\n",
    "**Target Users:** ___________________________\n",
    "\n",
    "### B. Gradio prototype summary\n",
    "\n",
    "**Completed features:**\n",
    "- [ ] Basic interface with inputs/outputs\n",
    "- [ ] LLM integration\n",
    "- [ ] Chatbot with conversation history\n",
    "- [ ] Explainability display\n",
    "- [ ] User feedback mechanism\n",
    "- [ ] Deployed to Hugging Face Spaces\n",
    "\n",
    "**LLM choice:** ___________________________\n",
    "\n",
    "**Key customizations made:** ___________________________\n",
    "\n",
    "**Deployment URL:** ___________________________\n",
    "\n",
    "### C. Streamlit dashboard summary\n",
    "\n",
    "**Completed features:**\n",
    "- [ ] Multi-page structure\n",
    "- [ ] Chat interface with memory\n",
    "- [ ] State management for history\n",
    "- [ ] Caching for expensive operations\n",
    "- [ ] Explainability analysis page\n",
    "- [ ] Feedback collection and visualization\n",
    "- [ ] Performance monitoring\n",
    "- [ ] Documentation page\n",
    "- [ ] Deployed to Streamlit Cloud\n",
    "\n",
    "**Pages implemented:**\n",
    "1. ___________________________\n",
    "2. ___________________________\n",
    "3. ___________________________\n",
    "4. ___________________________\n",
    "5. ___________________________\n",
    "\n",
    "**State management approach:** ___________________________\n",
    "\n",
    "**Caching strategy:** ___________________________\n",
    "\n",
    "**Deployment URL:** ___________________________\n",
    "\n",
    "### D. Explainability integration\n",
    "\n",
    "**Method used:** [ ] SHAP [ ] LIME [ ] Both\n",
    "\n",
    "**Implementation details:** ___________________________\n",
    "\n",
    "**Visualizations created:** ___________________________\n",
    "\n",
    "**User value provided:** ___________________________\n",
    "\n",
    "### E. Feedback mechanism\n",
    "\n",
    "**Feedback type:** ___________________________\n",
    "\n",
    "**Storage approach:** ___________________________\n",
    "\n",
    "**Analytics implemented:** ___________________________\n",
    "\n",
    "**Total feedback collected (during testing):** ___________________________\n",
    "\n",
    "### F. Team collaboration\n",
    "\n",
    "**Role distribution:**\n",
    "\n",
    "| Team Member | Primary Role | Key Contributions |\n",
    "|------------|--------------|-------------------|\n",
    "| | | |\n",
    "| | | |\n",
    "| | | |\n",
    "| | | |\n",
    "\n",
    "**Collaboration tools used:** ___________________________\n",
    "\n",
    "**Challenges encountered:** ___________________________\n",
    "\n",
    "**Solutions implemented:** ___________________________\n",
    "\n",
    "### G. Technical decisions\n",
    "\n",
    "**Framework comparison insights:**\n",
    "\n",
    "**When to use Gradio:**\n",
    "- ___________________________\n",
    "- ___________________________\n",
    "\n",
    "**When to use Streamlit:**\n",
    "- ___________________________\n",
    "- ___________________________\n",
    "\n",
    "**Most valuable feature learned:** ___________________________\n",
    "\n",
    "**Most challenging aspect:** ___________________________\n",
    "\n",
    "### H. Deployment experience\n",
    "\n",
    "**Platform(s) used:** ___________________________\n",
    "\n",
    "**Deployment challenges:** ___________________________\n",
    "\n",
    "**Security measures implemented:** ___________________________\n",
    "\n",
    "**Performance observations:** ___________________________\n",
    "\n",
    "### I. Future improvements\n",
    "\n",
    "**Short-term enhancements:**\n",
    "1. ___________________________\n",
    "2. ___________________________\n",
    "3. ___________________________\n",
    "\n",
    "**Long-term vision:**\n",
    "___________________________\n",
    "\n",
    "**Scalability considerations:**\n",
    "___________________________\n",
    "\n",
    "### J. Learning outcomes\n",
    "\n",
    "**Key concepts mastered:**\n",
    "- [ ] Gradio interface development\n",
    "- [ ] Streamlit dashboard creation\n",
    "- [ ] State management in reactive apps\n",
    "- [ ] Caching strategies for performance\n",
    "- [ ] Multi-page application architecture\n",
    "- [ ] Deployment to cloud platforms\n",
    "- [ ] API key security best practices\n",
    "- [ ] User feedback collection\n",
    "- [ ] Explainability integration\n",
    "\n",
    "**Most valuable insight:** ___________________________\n",
    "\n",
    "**How this applies to real projects:** ___________________________\n",
    "\n",
    "**Skills gained:** ___________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team submission checklist\n",
    "\n",
    "Before submitting, ensure your team has completed:\n",
    "\n",
    "**Planning and setup:**\n",
    "- [ ] Team roles clearly defined and documented\n",
    "- [ ] Application use case and target users identified\n",
    "- [ ] Development environment configured\n",
    "- [ ] All required libraries installed\n",
    "\n",
    "**Gradio prototype:**\n",
    "- [ ] `gradio_app_template.py` customized for your use case\n",
    "- [ ] LLM integrated (OpenAI/Bedrock/Ollama/Transformers)\n",
    "- [ ] Chatbot interface functional with conversation memory\n",
    "- [ ] Explainability display implemented\n",
    "- [ ] User feedback mechanism (thumbs up/down) working\n",
    "- [ ] Tested with multiple example conversations\n",
    "- [ ] Deployed to Hugging Face Spaces (or local deployment documented)\n",
    "- [ ] API keys properly secured (no hardcoded secrets)\n",
    "\n",
    "**Streamlit dashboard:**\n",
    "- [ ] `streamlit_app_template.py` customized for your use case\n",
    "- [ ] Multi-page structure implemented\n",
    "- [ ] Chat page with conversation history working\n",
    "- [ ] State management implemented for persistence\n",
    "- [ ] Caching applied to expensive operations\n",
    "- [ ] Explainability analysis page functional\n",
    "- [ ] Feedback dashboard with visualizations\n",
    "- [ ] Monitoring page showing metrics\n",
    "- [ ] Documentation page completed with team information\n",
    "- [ ] Tested thoroughly (state, caching, all pages)\n",
    "- [ ] Deployed to Streamlit Cloud (or local deployment documented)\n",
    "- [ ] API keys secured via secrets management\n",
    "\n",
    "**Explainability integration:**\n",
    "- [ ] SHAP or LIME implemented\n",
    "- [ ] Visualizations display correctly\n",
    "- [ ] Explanations integrated with LLM responses\n",
    "- [ ] Users can understand model decisions\n",
    "\n",
    "**Documentation:**\n",
    "- [ ] Team information documented\n",
    "- [ ] Role distribution explained\n",
    "- [ ] Technical decisions justified\n",
    "- [ ] Deployment instructions provided\n",
    "- [ ] User guide included\n",
    "- [ ] Code well-commented\n",
    "\n",
    "**Testing:**\n",
    "- [ ] Both applications tested with various inputs\n",
    "- [ ] Edge cases handled (empty input, long text, errors)\n",
    "- [ ] Performance acceptable (caching working)\n",
    "- [ ] Feedback mechanism collecting data properly\n",
    "- [ ] Deployment working (accessible URLs)\n",
    "\n",
    "**Submission files:**\n",
    "- [ ] `TeamName_Module15_Project.ipynb` (this completed notebook)\n",
    "- [ ] `TeamName_Module15_GradioApp.py` (customized Gradio app)\n",
    "- [ ] `TeamName_Module15_StreamlitApp.py` (customized Streamlit app)\n",
    "- [ ] `requirements.txt` (all dependencies listed)\n",
    "- [ ] `README.md` (deployment and usage instructions)\n",
    "- [ ] `deployment_urls.txt` (links to deployed applications)\n",
    "\n",
    "**Estimated completion time: 80 minutes (per team member)**\n",
    "\n",
    "---\n",
    "\n",
    "## Resources and next steps\n",
    "\n",
    "**Official documentation:**\n",
    "- Gradio: https://www.gradio.app/docs\n",
    "- Streamlit: https://docs.streamlit.io/\n",
    "- Hugging Face Spaces: https://huggingface.co/docs/hub/spaces\n",
    "- Streamlit Community Cloud: https://streamlit.io/cloud\n",
    "\n",
    "**Deployment platforms:**\n",
    "- Hugging Face Spaces: https://huggingface.co/spaces\n",
    "- Streamlit Cloud: https://share.streamlit.io\n",
    "- Docker Hub: https://hub.docker.com/\n",
    "\n",
    "**Explainability libraries:**\n",
    "- SHAP: https://shap.readthedocs.io/\n",
    "- LIME: https://lime-ml.readthedocs.io/\n",
    "\n",
    "**LLM integration:**\n",
    "- OpenAI: https://platform.openai.com/docs\n",
    "- LangChain: https://python.langchain.com/docs\n",
    "- Hugging Face Transformers: https://huggingface.co/docs/transformers\n",
    "\n",
    "---\n",
    "\n",
    "**End of Module 15 Team Project**\n",
    "\n",
    "**Good luck with your Trustworthy AI Explainer Dashboard!** \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
